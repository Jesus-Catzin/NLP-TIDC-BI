{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "leading-billy",
   "metadata": {},
   "source": [
    "# Tweets Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lovely-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import mysql.connector\n",
    "import re\n",
    "from Preprocessor import Preprocessor\n",
    "from mysql.connector import Error\n",
    "from scrappingLocation import scrap_latlon\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "import keras.preprocessing.text as kpt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "posted-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_dbs(host, user, password):\n",
    "    '''\n",
    "    Funtion to connect to the data base\n",
    "    \n",
    "    Inputs:\n",
    "        host: the host where is the data base\n",
    "        user: the username\n",
    "        password: the password \n",
    "    '''\n",
    "    try:\n",
    "        conn = mysql.connector.connect(host=host, user=user,  \n",
    "                            password=password)#give ur username, password\n",
    "        if conn.is_connected():\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"CREATE DATABASE databasecovid\")\n",
    "            print(\"Database is created\")\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to MySQL\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "white-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tweets_db():\n",
    "    '''\n",
    "    Funtion to add the tweets to the table from the data base\n",
    "    '''\n",
    "    mydb = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        passwd=\"\",\n",
    "        database=\"databasecovid\"\n",
    "    )\n",
    "    if mydb.is_connected():\n",
    "        '''\n",
    "        Check if this table exits. If not, then create a new one.\n",
    "        '''\n",
    "        mycursor = mydb.cursor()\n",
    "        mycursor.execute(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_name = '{0}'\n",
    "            \"\"\".format(table_name))\n",
    "        if mycursor.fetchone()[0] != 1:\n",
    "            mycursor.execute(\"CREATE TABLE {} ({})\".format(table_name, attributes))\n",
    "            mydb.commit()\n",
    "        mycursor.close()\n",
    "        return mydb\n",
    "    else:\n",
    "        return mydb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "freelance-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamListener(tweepy.StreamListener): #Class for streaming\n",
    "    '''\n",
    "    Class to get streaming tweets\n",
    "    '''\n",
    "    def __init__(self, no_tweets=100):\n",
    "        super().__init__()\n",
    "        self.no_tweets = no_tweets\n",
    "    \n",
    "    def on_status(self, status): #Extract tweets text\n",
    "        preprocessor = Preprocessor()\n",
    "        if status.retweeted:\n",
    "            # Avoid retweeted info, and only original tweets will be received\n",
    "            return True\n",
    "        # Extract attributes from each tweet\n",
    "        text = status.text\n",
    "        text = preprocessor.cleaning(text)# Pre-processing the text\n",
    "        if text == '': # pass the empty text\n",
    "            return True\n",
    "        \n",
    "        # some preproccessing to the pure tweet \n",
    "        pure_text = status.text\n",
    "        pure_text = preprocessor.clean_emojis(pure_text)\n",
    "        pure_text = preprocessor.re_process(pure_text)\n",
    "        \n",
    "        # get the creation date\n",
    "        created_at = status.created_at\n",
    "        \n",
    "        # get the location user\n",
    "        user_location = preprocessor.clean_emojis(status.user.location)\n",
    "        if user_location == None: # if the locations is none, pass\n",
    "            return True\n",
    "        \n",
    "        # preproccessing to location \n",
    "        user_location_check = re.sub(r'[^A-Za-z\\s]+', '', user_location).lower()\n",
    "        \n",
    "        # check if the location has the word \"here\"\n",
    "        list_check = re.findall(r'\\bhere\\b', user_location_check)\n",
    "        \n",
    "        if list_check: # pass if the location has the word \"here\"\n",
    "            return True\n",
    "        \n",
    "        # predict the word if it is toxic or no \n",
    "        split_word = [word.split() for word in [text]]\n",
    "        split_word = [[word for word in split_word[0] if word in tokenizer.word_index]]\n",
    "        tokenizer.fit_on_texts(split_word) \n",
    "        X_data = tokenizer.texts_to_sequences(split_word)\n",
    "        X_data = pad_sequences(X_data, padding = 'pre', maxlen = 20)\n",
    "        toxic = int((model.predict(X_data) > 0.5).astype(int)[0][0])\n",
    "        \n",
    "        \n",
    "        # if there is a error in the code from scrappingLatLon\n",
    "        try:\n",
    "            lat_lon = scrap_latlon(user_location)\n",
    "        except:\n",
    "            return True\n",
    "        if not lat_lon:\n",
    "            return True\n",
    "        # get latitude and longitude\n",
    "        latitude = lat_lon[0]\n",
    "        longitude = lat_lon[1]\n",
    "        \n",
    "        # Store all data in MySQL\n",
    "        if mydb.is_connected():\n",
    "            mycursor = mydb.cursor()\n",
    "            sql = f\"INSERT INTO {table_name} (created_at, pure_tweet, pro_tweet, toxic, user_location, longitude, latitude) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "            val = (created_at, pure_text, text, toxic, user_location, longitude, latitude)\n",
    "            mycursor.execute(sql, val)\n",
    "            mydb.commit()\n",
    "            mycursor.close()\n",
    "        \n",
    "        # stop to avoid errors\n",
    "        time.sleep(5)\n",
    "        # stop when the number of tweets to search is 0\n",
    "        if self.no_tweets == 0:\n",
    "            return False\n",
    "        else:\n",
    "            self.no_tweets -= 1\n",
    "    \n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        #Since Twitter API has rate limits, stop srcraping data as it exceed to the thresold.\n",
    "        \n",
    "        if status_code == 420:\n",
    "            # return False to disconnect the stream\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indoor-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and store the tokenizer \n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    \n",
    "# open and store the model \n",
    "model = load_model('FinalModel.model', compile = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "royal-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words to search into twitter\n",
    "search_words = ['Coronavirus','Covid19','covid','covid19','coronavirus','Covid-19','covid-19','Covid']\n",
    "# name of the table where will be the stored\n",
    "table_name = 'tweetsCovid'\n",
    "# attributes to the columns \n",
    "attributes = \"created_at DATETIME, pure_tweet VARCHAR(255), pro_tweet VARCHAR(255), toxic INT, user_location VARCHAR(255), longitude DOUBLE, latitude DOUBLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "equipped-verification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while connecting to MySQL 1007 (HY000): Can't create database 'databasecovid'; database exists\n"
     ]
    }
   ],
   "source": [
    "# connect to the database\n",
    "connect_dbs('localhost', 'root', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polish-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the API kyas and access\n",
    "login = pd.read_csv('login.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "circular-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  store the keys and access\n",
    "api_key = login['api_key'][0]\n",
    "api_key_secret = login['api_key_secret'][0]\n",
    "access_token = login['access_token'][0]\n",
    "access_token_secret = login['access_token_secret'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "different-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat the api\n",
    "authenticate = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "authenticate.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(authenticate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tweets from twitter, filter: english and tweets with covid \n",
    "mydb = add_tweets_db()\n",
    "n_times = 5\n",
    "while n_times >= 1:\n",
    "    try:\n",
    "        myStreamListener = MyStreamListener(no_tweets=150)\n",
    "        myStream = tweepy.Stream(auth = api.auth, listener = myStreamListener)\n",
    "        myStream.filter(languages=[\"en\"], track = search_words)\n",
    "    except:\n",
    "        n_times -= 1\n",
    "        print('Lost connection')\n",
    "    else:\n",
    "        n_times -= 1\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marked-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"localhost\", \n",
    "    user=\"root\",\n",
    "    passwd=\"\",\n",
    "    database=\"databasecovid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "southern-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to DataFame\n",
    "df = pd.read_sql('SELECT pure_tweet, toxic FROM {}'.format(table_name), con = db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "diagnostic-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get the no-toxic tweets\n",
    "df = df[df['toxic'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "finite-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to model Generative\n",
    "df.to_csv('NoToxicTweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
